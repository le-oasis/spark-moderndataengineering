{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437fcd68-831b-48d6-87af-3f905149da90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Joins!\n",
    "\n",
    "- More likely, you will have to stitch data from a few different sources together in order to create the data representation needed to solve the problem at hand.\n",
    "\n",
    "- Joins are common within the data pipeline as a solution to combining data. \n",
    "- These workflows fall under the umbrella of the ETL and can be used whenever you need to strategically combine and transform multiple sources of data into a single consolidated view that can be used to answer more targeted problems.\n",
    "- For example, say we were tasked with creating a job that generates the current available occupancy data for our coffee shops. For the sake of the exercise, let’s say we already have a source of data that emits the number of occupied seats per coffee shop. \n",
    "- We can use this data to join with our coffee stores data to create a new view that we can query to find which store can seat a variable sized party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379d8f6d-3c9b-49fd-a62f-f8adaf6a579a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+\n",
      "|name|capacity|opens|closes|\n",
      "+----+--------+-----+------+\n",
      "|   a|      24|    8|    20|\n",
      "|   b|      36|    7|    21|\n",
      "|   c|      18|    5|    23|\n",
      "+----+--------+-----+------+\n",
      "\n",
      "+---------+---------+\n",
      "|storename|occupants|\n",
      "+---------+---------+\n",
      "|        a|        8|\n",
      "|        b|       20|\n",
      "|        c|       16|\n",
      "|        d|       55|\n",
      "|        e|        8|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "# Delta is a storage layer for data lakes\n",
    "from delta.tables import * \n",
    "# DeltaTable is the main class for Delta tables\n",
    "from delta.tables import DeltaTable \n",
    "\n",
    "# Initialize SparkSession\n",
    "# Create a SparkSession and set the extraClassPath configuration\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "    .appName(\"StoreOccupants\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/*\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define the schema for the Store class\n",
    "store_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"capacity\", IntegerType(), True),\n",
    "    StructField(\"opens\", IntegerType(), True),\n",
    "    StructField(\"closes\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a list of Row objects\n",
    "stores = [\n",
    "    Row(\"a\", 24, 8, 20),\n",
    "    Row(\"b\", 36, 7, 21),\n",
    "    Row(\"c\", 18, 5, 23)\n",
    "]\n",
    "\n",
    "# Create a PySpark DataFrame from the Row objects and schema\n",
    "stores_sdf = spark.createDataFrame(stores, store_schema)\n",
    "stores_sdf.show()\n",
    "\n",
    "# Define the schema for the Occupants\n",
    "schema = StructType([\n",
    "  StructField(\"storename\", StringType(), True),\n",
    "  StructField(\"occupants\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a PySpark DataFrame from the occupants sequence\n",
    "occupants_sdf = spark.createDataFrame([\n",
    "  (\"a\", 8),\n",
    "  (\"b\", 20),\n",
    "  (\"c\", 16),\n",
    "  (\"d\", 55),\n",
    "  (\"e\", 8)\n",
    "], schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "occupants_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3a36d-71ce-4dfb-92c8-902f4802ce9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02669868-813d-4699-bd54-28e39d8227a1",
   "metadata": {},
   "source": [
    "### Create SQL view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eff0c62-dda1-4a8a-9bee-adcf179ef155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register the stores and store_occupants DataFrames as temporary views\n",
    "stores_sdf.createOrReplaceTempView(\"stores\")\n",
    "occupants_sdf.createOrReplaceTempView(\"store_occupants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e4357-7bb6-4242-bcc5-1d3717b2c22c",
   "metadata": {},
   "source": [
    "## Inner Join\n",
    "\n",
    "- The inner join is the simplest to understand and just so happens to also be the default join operation in Spark (given this is usually how people want to join data). \n",
    "- The inner join works by selecting only the rows that meet the join selection criteria across both sides of the data being joined.\n",
    "- Inner joins simply ignore all rows that don’t have a matching join condition. Next up, let’s look at the right join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ddb4af-2ffa-4a73-a81b-a3df7c4698e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+---------+\n",
      "|name|capacity|opens|closes|storename|occupants|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "|   a|      24|    8|    20|        a|        8|\n",
      "|   b|      36|    7|    21|        b|       20|\n",
      "|   c|      18|    5|    23|        c|       16|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Execute the SparkSQL query\n",
    "innerjoin_df = spark.sql(\"SELECT * FROM stores INNER JOIN store_occupants ON stores.name = store_occupants.storename\")\n",
    "\n",
    "# Show the joined DataFrame\n",
    "innerjoin_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea0111-add3-4b26-adfd-b44b3ff43488",
   "metadata": {},
   "source": [
    "- The result of our join operation is a new DataFrame that combines all the columns of our two data sources where the join criteria is met. \n",
    "- In this case, that’s where there is a matching store name across both data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b790b-b55e-498c-8577-4f51666375c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cced074d-203c-432f-8d55-b3c30003a24c",
   "metadata": {},
   "source": [
    "## Right Join\n",
    "\n",
    "- The right join, or right outer join, returns all rows from the right-side data source explicitly joining all rows where the selection criteria is met with the left side of the data. \n",
    "- When and the data doesn’t match, it will insert null values instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33329a60-ca8b-4c9d-a694-73802cc501d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+\n",
      "|name|capacity|opens|closes|occupants|\n",
      "+----+--------+-----+------+---------+\n",
      "|   a|      24|    8|    20|        8|\n",
      "|   b|      36|    7|    21|       20|\n",
      "|   c|      18|    5|    23|       16|\n",
      "|null|    null| null|  null|       55|\n",
      "|null|    null| null|  null|        8|\n",
      "+----+--------+-----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute the SparkSQL query\n",
    "rightjoin_df = spark.sql(\"SELECT stores.*, store_occupants.occupants FROM stores RIGHT JOIN store_occupants ON stores.name = store_occupants.storename\")\n",
    "\n",
    "# Show the joined DataFrame\n",
    "rightjoin_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb5be0-10eb-45f6-b6fa-3d65b78546ad",
   "metadata": {},
   "source": [
    "**DataframeAPI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33314c7-9414-4bf0-88d5-bc5521269a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+---------+\n",
      "|name|capacity|opens|closes|storename|occupants|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "|   a|      24|    8|    20|        a|        8|\n",
      "|   b|      36|    7|    21|        b|       20|\n",
      "|   c|      18|    5|    23|        c|       16|\n",
      "|null|    null| null|  null|        d|       55|\n",
      "|null|    null| null|  null|        e|        8|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `stores_sdf` is our stores data DataFrame\n",
    "right_joined = stores_sdf.join(occupants_sdf, stores_sdf[\"name\"] == occupants_sdf[\"storename\"], \"right\")\n",
    "right_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8bbc9f-440c-432c-82a0-619813b854d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410721f2-0d99-4a57-b35c-698863850233",
   "metadata": {},
   "source": [
    "- In a right join, all rows from the right table (store_occupants in this case) are included in the result, along with matching rows from the left table (stores in this case).\n",
    "- If there are no matching rows in the left table for a given row in the right table, then the result will contain NULL values for the columns of the left table.\n",
    "\n",
    "**Therefore, in this query, the store_occupants table is on the right side of the join, and the stores table is on the left side.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b6006-7784-4f37-b972-85d54ad61b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
