{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6fa3b3-c4f1-44d9-bc0d-63bc00053c42",
   "metadata": {},
   "source": [
    "Bridging Spark SQL with JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcdec3c-f6ee-43a7-9c10-0a9320d54129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "# Delta is a storage layer for data lakes\n",
    "from delta.tables import * \n",
    "# DeltaTable is the main class for Delta tables\n",
    "from delta.tables import DeltaTable \n",
    "\n",
    "# Initialize SparkSession\n",
    "# Create a SparkSession and set the extraClassPath configuration\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "    .appName(\"BridgeMySQL\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/*\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f5b14-86fb-408f-9c32-7b90516a0dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82508375-c9d6-4b90-bc10-a6429093ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.hadoop.fs.s3a.connection.ssl.enabled = false\n",
      "spark.hadoop.fs.s3a.path.style.access = true\n",
      "spark.sql.hive.metastore.schema.verification = true\n",
      "spark.driver.extraClassPath = /home/jovyan/work/jars/*\n",
      "spark.hadoop.fs.s3a.block.size = 1000M\n",
      "spark.sql.hive.metastore.schema.verification.record.version = true\n",
      "spark.serializer = org.apache.spark.serializer.KryoSerializer\n",
      "spark.hadoop.fs.s3a.endpoint = s3.amazonaws.com\n",
      "spark.app.name = BridgeMySQL\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.submit.deployMode = client\n",
      "spark.sql.hive.metastore.version = 2.3.9\n",
      "spark.master = local[1]\n",
      "spark.hadoop.fs.s3a.secret.key = \n",
      "spark.sql.hive.metastore.sharedPrefixes = org.mariadb.jdbc,com.mysql.cj.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc\n",
      "spark.executor.id = driver\n",
      "spark.driver.host = myjupyter\n",
      "spark.hadoop.fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "spark.sql.catalogImplementation = hive\n",
      "spark.sql.hive.metastore.jars = builtin\n",
      "spark.rdd.compress = True\n",
      "spark.driver.port = 36515\n",
      "spark.sql.extensions = io.delta.sql.DeltaSparkSessionExtension,org.apache.sedona.sql.SedonaSqlExtensions\n",
      "spark.hadoop.fs.s3a.access.key = \n",
      "spark.submit.pyFiles = \n",
      "spark.app.startTime = 1677024275043\n",
      "spark.sql.warehouse.dir = file:/usr/local/spark/sql/warehouse\n",
      "spark.kryo.registrator = org.apache.sedona.core.serde.SedonaKryoRegistrator\n",
      "spark.ui.showConsoleProgress = true\n",
      "spark.sql.catalog.spark_catalog = org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "spark.app.id = local-1677024276020\n"
     ]
    }
   ],
   "source": [
    "# Retrieving and Printing Spark Configuration Settings\n",
    "conf = spark.sparkContext.getConf().getAll()\n",
    "conf_dict = dict(conf)\n",
    "for key, value in conf_dict.items():\n",
    "    print(\"{} = {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2d6507b-5acb-4d93-9270-9fc594d2002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------------+----------+-------------+--------------------+--------+----------+-------+--------------+--------------------+------------+----------+--------+----------+----------+------------+-----------+\n",
      "|  id|     eventType|        subject| eventTime|customer_name|             address|    city|postalcode|country|         phone|               email|product_name|order_date|currency|order_mode|sale_price|order_number|dataVersion|\n",
      "+----+--------------+---------------+----------+-------------+--------------------+--------+----------+-------+--------------+--------------------+------------+----------+--------+----------+----------+------------+-----------+\n",
      "|3001|recordInserted|ecomm/customers|2021-01-01|   Julie Rich|Ap #255-3031 Dui ...|Billings|     80834|    USA|1-528-884-4331|Donec.felis@neque...|   microwave|18/05/2021|     EUR|       NEW|     32.34|         385|        1.0|\n",
      "|3002|recordInserted|ecomm/customers|2021-01-01|Ratnali Kumar|Ap #476-7527 Aene...|Bhilwara|    827484|  India|+91 0952796185|Donec.feugiat@fel...|   microwave|18/05/2021|     USD|       NEW|     44.45|         386|        1.0|\n",
      "+----+--------------+---------------+----------+-------------+--------------------+--------+----------+-------+--------------+--------------------+------------+----------+--------+----------+----------+------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- eventType: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- postalcode: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- order_mode: string (nullable = true)\n",
      " |-- sale_price: string (nullable = true)\n",
      " |-- order_number: string (nullable = true)\n",
      " |-- dataVersion: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# read csv with options\n",
    "# Read the data with the specified schema and create a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"/home/jovyan/work/data/ecomm_orders.csv\")\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "\n",
    "# convert to avro\n",
    "df.write.format(\"com.databricks.spark.avro\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "   .save(\"/home/jovyan/work/data/output_file.avro\")\n",
    "\n",
    "# convert to parquet\n",
    "df.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/data/ecomm_orders.parquet\")\n",
    "\n",
    "# convert to json\n",
    "df.write.mode(\"overwrite\").json(\"/home/jovyan/work/data/ecomm_orders.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22118fc8-b72f-4b99-97eb-cdd29df4fd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e00965-ac7c-4716-8340-712104d3956f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67a5b4df-98ec-4a9b-b082-f65bc3e95183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jdbcDriver = spark.conf.get(\"spark.jdbc.driver.class\", \"org.mariadb.jdbc.Driver\")\n",
    "dbHost = spark.conf.get(\"spark.jdbc.host\",\"mysql\")\n",
    "dbPort = spark.conf.get(\"spark.jdbc.port\", \"3306\")\n",
    "defaultDb = spark.conf.get(\"spark.jdbc.default.db\", \"default\")\n",
    "dbTable = spark.conf.get(\"spark.jdbc.table\", \"customers\")\n",
    "dbUser = spark.conf.get(\"spark.jdbc.user\", \"dataeng\")\n",
    "dbPass = spark.conf.get(\"spark.jdbc.password\", \"dataengineering_user\")\n",
    "\n",
    "connectionUrl = f\"jdbc:mysql://{dbHost}:{dbPort}/{defaultDb}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89469aff-73d2-4139-a793-1c8ddfa55dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94df5f-cae4-4c3c-bc3c-c8a559e1de25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
